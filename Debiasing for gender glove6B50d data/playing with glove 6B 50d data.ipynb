{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playing around with glove 6B 50-d data\n",
    "\n",
    "\n",
    "- **list of things to play with the data**\n",
    " - Finding the cosine similarity\n",
    " - Analogy game. (man->woman, king: ?)\n",
    " - Debiasing word vectors\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('./data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u,v):\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    \n",
    "    cosine_similarity = dot/(norm_u*norm_v)\n",
    "    return cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine sim of name , title 0.6283471929719754\n",
      "Cosine sim of title , name 0.6283471929719754\n"
     ]
    }
   ],
   "source": [
    "name = word_to_vec_map['name']\n",
    "title = word_to_vec_map['title']\n",
    "\n",
    "print('Cosine sim of name , title', cosine_similarity(name, title))\n",
    "\n",
    "#as cosine is symetric a.b = b.a cosine sim bt a, b = b, a\n",
    "\n",
    "print('Cosine sim of title , name', cosine_similarity(title, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_game(word_a, word_b, word_c, word_to_vec_map):\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    \n",
    "    max_cosine_sim = -100 #lets say very negative\n",
    "    best_word = None # we don't know for now\n",
    "    tick = time.time()\n",
    "    for w in words:\n",
    "        if w in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        \n",
    "        cosine_sim = cosine_similarity(e_a-e_b, e_c-word_to_vec_map[w])\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "    tock = time.time()\n",
    "    time_taken = tock-tick\n",
    "            \n",
    "    return best_word, time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman->sexy,man->retro,4.4205710887908936\n",
      "man->hot,woman->chill,4.115301132202148\n",
      "boy->young,girl->fellow,4.053421258926392\n",
      "name->title,beer->100m,4.028311729431152\n",
      "pain->sorrow,laugh->jodhaa,4.085860967636108\n",
      "lion->deer,zebra->white-tailed,4.098408937454224\n"
     ]
    }
   ],
   "source": [
    "#print(analogy_game('girl', 'cute', 'boy', word_to_vec_map))\n",
    "\n",
    "triads = [('woman','sexy', 'man'), ('man','hot', 'woman'), ('boy', 'young', 'girl'),('name', 'title', 'beer'),\n",
    "         ('pain', 'sorrow', 'laugh'), ('lion','deer','zebra')]\n",
    "#PLAY AROUND BY ADDING MORE TO THE LIST LIKE NAME->TITLE, BEER->\n",
    "for triad in triads:\n",
    "    best_word , time_taken = analogy_game(*triad, word_to_vec_map)\n",
    "    print('{}->{},{}->{},{}'.format(*triad, best_word, time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiasing Word Vectors for gender\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07656667  0.34967667 -0.40057667 -0.03130333  0.0088      0.72586333\n",
      "  0.10256     0.14906333  0.4780662  -0.22850987  0.05957667 -0.68663\n",
      "  0.62210033  0.10395     0.17747667  0.09556867 -0.49258333 -0.17066233\n",
      "  0.46930033  0.02196333  0.28145667  0.50513333  0.17144733  0.40154767\n",
      "  0.24039333  0.1646     -0.17984667  0.24042667  0.05689333 -0.31423\n",
      " -0.10933333  0.26355967  0.06100667 -0.01156405 -0.12236333 -0.188245\n",
      " -0.13215057 -0.068186    0.05624667 -0.29555567 -0.09669533 -0.29559667\n",
      "  0.62465867 -0.40130167  0.03330667 -0.24831667  0.26381667 -0.28738333\n",
      "  0.03020433  0.054106  ]\n"
     ]
    }
   ],
   "source": [
    "g1 = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "g2 = word_to_vec_map['mother'] -word_to_vec_map['father']\n",
    "g3 = word_to_vec_map['girl'] - word_to_vec_map['boy']\n",
    "g = g1+g2+g3\n",
    "g = g/3\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check what words means what in bias axis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "john -0.30873091089769916\n",
      "marie 0.34257515107827113\n",
      "sophie 0.4116200252265307\n",
      "ronaldo -0.290839785117324\n",
      "priya 0.19646793448600458\n",
      "rahul -0.19492147638633417\n",
      "danielle 0.2923957653171286\n",
      "reza -0.1679382162425299\n",
      "katy 0.31132430605664335\n",
      "yasmin 0.19658379893678699\n"
     ]
    }
   ],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "lipstick 0.41366815126252465\n",
      "guns -0.08755154639507805\n",
      "science -0.05837425964384823\n",
      "arts 0.011760468125783753\n",
      "literature 0.02316946789375169\n",
      "warrior -0.16564638100307952\n",
      "doctor 0.0772141272665667\n",
      "tree 0.03538042107098228\n",
      "receptionist 0.30167259871100655\n",
      "technology -0.16192108462558172\n",
      "fashion 0.1416547219136271\n",
      "teacher 0.10545901736578715\n",
      "engineer -0.2263994415742677\n",
      "pilot -0.03699357317847414\n",
      "computer -0.16821031921735138\n",
      "singer 0.2009300079322625\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neutralize.png\" style=\"width:800px;height:400px;\"> \n",
    "\n",
    "<img src=\"images/neutral.png\" style=\"width:800px;height:400px;\"> \n",
    "\n",
    "What I observed is - \n",
    " - receptionist\n",
    " - technology\n",
    " - fashion\n",
    " - teacher \n",
    " - engineer\n",
    " - computer \n",
    " - singer \n",
    " \n",
    " these are the biased term towards some gender\n",
    " \n",
    " Equations : \n",
    " \n",
    " \n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\\tag{2}$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}\\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map. (≈ 1 line)\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula give above. (≈ 1 line)\n",
    "    e_biascomponent = (np.dot(e, g)/np.square(np.linalg.norm(g)))*g\n",
    " \n",
    "    # Neutralize e by substracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection. (≈ 1 line)\n",
    "    word = e - e_biascomponent\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.30167259871100655\n",
      "cosine similarity between receptionist and g, after neutralizing:  -2.6739863413957255e-17\n",
      "\n",
      "\n",
      "cosine similarity between technology and g, before neutralizing:  -0.16192108462558172\n",
      "cosine similarity between technology and g, after neutralizing:  -2.853148170277109e-17\n",
      "\n",
      "\n",
      "cosine similarity between fashion and g, before neutralizing:  0.1416547219136271\n",
      "cosine similarity between fashion and g, after neutralizing:  6.950184658989011e-17\n",
      "\n",
      "\n",
      "cosine similarity between teacher and g, before neutralizing:  0.10545901736578715\n",
      "cosine similarity between teacher and g, after neutralizing:  -3.4264800465992284e-17\n",
      "\n",
      "\n",
      "cosine similarity between engineer and g, before neutralizing:  -0.2263994415742677\n",
      "cosine similarity between engineer and g, after neutralizing:  7.258856232914164e-17\n",
      "\n",
      "\n",
      "cosine similarity between computer and g, before neutralizing:  -0.16821031921735138\n",
      "cosine similarity between computer and g, after neutralizing:  -5.2811565087643103e-17\n",
      "\n",
      "\n",
      "cosine similarity between singer and g, before neutralizing:  0.2009300079322625\n",
      "cosine similarity between singer and g, after neutralizing:  -1.0079838050009539e-16\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['receptionist', 'technology', 'fashion', 'teacher','engineer', 'computer','singer']\n",
    "for word in words:\n",
    "    print(\"cosine similarity between \" + word + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[word], g))\n",
    "\n",
    "    word_debiased = neutralize(word, g, word_to_vec_map)\n",
    "    \n",
    "    print(\"cosine similarity between \" + word + \" and g, after neutralizing: \", cosine_similarity(word_debiased, g))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalising by gender - Gender equality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/equalize10.png\" style=\"width:800px;height:400px;\"> \n",
    "\n",
    "**Equations:**\n",
    "\n",
    "The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are: \n",
    "\n",
    "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}\\tag{4}$$ \n",
    "\n",
    "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{5}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B} \\tag{6}$$\n",
    "\n",
    "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{7}$$ \n",
    "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "\\tag{8}$$\n",
    "\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||} \\tag{9}$$\n",
    "\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||} \\tag{10}$$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} \\tag{11}$$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} \\tag{12}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):    \n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    mu = (e_w1 + e_w2)/2\n",
    "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
    "    mu_B = (np.dot(mu,bias_axis)/np.square(np.linalg.norm(bias_axis)))*bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)\n",
    "    e_w1B = (np.dot(e_w1, bias_axis)/np.square(np.linalg.norm(bias_axis)))*bias_axis\n",
    "    e_w2B = (np.dot(e_w2, bias_axis)/np.square(np.linalg.norm(bias_axis)))*bias_axis\n",
    "        \n",
    "    # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)\n",
    "    corrected_e_w1B = np.sqrt(np.linalg.norm(1- np.square(np.linalg.norm(mu_orth)))) * (e_w1B - mu_B)/(np.linalg.norm((e_w1-mu_orth)-mu_B))\n",
    "    corrected_e_w2B = np.sqrt(np.linalg.norm(1- np.square(np.linalg.norm(mu_orth)))) * (e_w2B - mu_B)/(np.linalg.norm((e_w2-mu_orth)-mu_B))\n",
    "\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "                                                                \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"girl\"], gender) =  0.447588899901815\n",
      "cosine_similarity(word_to_vec_map[\"boy\"], gender) =  0.15516909802751455\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  0.6370186893403987\n",
      "cosine_similarity(e2, gender) =  -0.6370186893403987\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"girl\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"girl\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"boy\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"boy\"], g))\n",
    "\n",
    "e1, e2 = equalize((\"girl\", \"boy\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
