{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_RNN_Building RNN .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVzBYbPkiCgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#libraries needed\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ5h-rbPjX5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#helper functions that we need \n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  \n",
        "def initialize_adam(parameters):\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)  \n",
        "    return v, s\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    L = len(parameters) // 2                 \n",
        "    v_corrected = {}                         \n",
        "    s_corrected = {}  \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
        "    return parameters, v, s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiHn9hyClVm9",
        "colab_type": "text"
      },
      "source": [
        "**basic RNN**\n",
        "\n",
        "\n",
        "1.   takes as input the previous activation and the current input. \n",
        "\n",
        "    calculate current activation At = tanh(np.dot(Waa, At-1) + np.dot(Wax, Xt) + bias_A)\n",
        "\n",
        "\n",
        "2.   take current activation and output y\n",
        "      \n",
        "      that is , Yt = softmax( np.dot(Wya, At) + bias_y)\n",
        "      \n",
        "3. Store At, At-1, Xt, parameters in cache\n",
        "\n",
        "4. return At, Yt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**What is important is dimensions , when and how , who got what!**\n",
        "\n",
        ">Xt.shape = (n_x, m)  \n",
        "\n",
        ">At.shape = (n_a, m)\n",
        "\n",
        ">Wax.shape = (n_a, n_x)\n",
        "\n",
        ">Waa.shape = (n_a, n_a)\n",
        "\n",
        ">Wya.shape = ( n_y, n_a)\n",
        "\n",
        ">b_a.shape = (n_a, 1)\n",
        "\n",
        ">b_y.shape = (n_y, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_XtKqtFkWLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_forward_block(xt, aprev, parameters):\n",
        "  #1. take back the parameters into Waa, Wax, Wya, ba, bx\n",
        "  Wax = parameters['Wax']\n",
        "  Waa = parameters['Waa']\n",
        "  Wya = parameters['Wya']\n",
        "  ba = parameters['ba']\n",
        "  by  = parameters['by']\n",
        "  \n",
        "  anext = np.tanh( np.dot(Wax, xt) + np.dot( Waa, aprev) + ba)\n",
        "  yhat = np.dot(Wya, anext) + by\n",
        "  \n",
        "  cache = (anext, aprev, parameters, xt)\n",
        "  \n",
        "  return yhat, anext, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clo5puEEvV2w",
        "colab_type": "text"
      },
      "source": [
        "that was a single block.\n",
        "\n",
        "now ,\n",
        "X = (X1, X2, ..., Xtx)\n",
        "\n",
        "thus tx such blocks will work.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now what we need: \n",
        "\n",
        "1. A0 - vector of zeros of shape (n_a, m)\n",
        "\n",
        "2. Loop over the rnn_forward tx times.\n",
        "3. update a_prev everytime.\n",
        "4. store the prediction in y\n",
        "5. add cache into the list  caches\n",
        "\n",
        "return a, y, caches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0_rmTnGrpu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_forward(x, A0, parameters):\n",
        "  n_x , m, T_x = x.shape\n",
        "  n_y, n_a = parameters['Wya'].shape\n",
        "  \n",
        "  #A0 = np.zeros((n_a, m))\n",
        "  \n",
        "  A = np.zeros((n_a, m, T_x))\n",
        "  Y = np.zeros((n_y, m, T_x))\n",
        "  \n",
        "  A_prev = A0\n",
        "  caches = []\n",
        "  \n",
        "  for i in range(T_x):\n",
        "    y_i, a_i, cache = rnn_forward_block(x[:,:,i], A_prev, parameters)\n",
        "    A_prev = a_i\n",
        "    \n",
        "    Y[:,:,i] = y_i\n",
        "    A[:,:,i] = a_i\n",
        "    caches.append(cache)\n",
        "    \n",
        "  caches = (caches, x)\n",
        "    \n",
        "  return Y, A, caches\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwMs9bQj1gHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c04b010d-0b3e-4752-c9f6-97b8235e16b9"
      },
      "source": [
        "#check that what you have done is correct\n",
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "y_pred,a, caches = rnn_forward(x, a0, parameters)\n",
        "print(\"a[4][1] = \", a[4][1])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
        "print(\"y_pred.shape = \", y_pred.shape)\n",
        "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
        "print(\"len(caches) = \", len(caches))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
            "a.shape =  (5, 10, 4)\n",
            "y_pred[1][3] = [ 1.82870192  1.98141611 -0.7803359   1.26637504]\n",
            "y_pred.shape =  (2, 10, 4)\n",
            "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yFVqJi1nIK6",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NoMge2v1sr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cell_forward(xt, aprev, cprev, parameters):\n",
        "  #parameters retrivel\n",
        "  \n",
        "  Wf = parameters['Wf']\n",
        "  bf = parameters['bf']\n",
        "  \n",
        "  Wu = parameters['Wu']\n",
        "  bu = parameters['bu']\n",
        "  \n",
        "  Wo = parameters['Wo']\n",
        "  bo = parameters['bo']\n",
        "  \n",
        "  Wc = parameters['Wc']\n",
        "  bc = parameters['bc']\n",
        "  \n",
        "  Wy = parameters['Wy']\n",
        "  by = parameters['by']\n",
        "  \n",
        "  nx, m = xt.shape\n",
        "  ny, na = Wy.shape\n",
        "  \n",
        "  concat = np.zeros((na+nx, m))\n",
        "  concat[:na, :] = aprev\n",
        "  concat[na:, :] = nx\n",
        "  \n",
        "  forget = sigmoid(np.dot(Wf, concat) + bf) #forget from previous memory cell\n",
        "  \n",
        "  ctilda = np.tanh(np.dot(Wc, concat) + bc)\n",
        "  update = sigmoid(np.dot(Wu, concat) + bu)\n",
        "  \n",
        "  cnext = np.multiply(forget, cprev) + np.multiply(update,ctilda) #new memory cell\n",
        "  \n",
        "  output = sigmoid(np.dot(Wo, concat) + bo)\n",
        "  anext = np.multiply(output, np.tanh(cnext) )\n",
        "  \n",
        "  y = softmax(np.dot(Wy, anext) + by)\n",
        "  \n",
        "  cache = (anext, cnext, aprev, cprev, forget, update, ctilda, output, xt, parameters)\n",
        "  \n",
        "  return anext, cnext, y, cache\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku2rTNht-dyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_forward(x, a0, parameters):\n",
        "  caches = []\n",
        "  nx, m, Tx = x.shape\n",
        "  ny, na = parameters['Wy'].shape\n",
        "  \n",
        "  A = np.zeros((na, m, Tx))\n",
        "  C = np.zeros((na, m, Tx))\n",
        "  Y = np.zeros((ny, m, Tx))\n",
        "  \n",
        "  aprev = a0\n",
        "  cprev = np.zeros(aprev.shape)\n",
        "  \n",
        "  for i in range(Tx):\n",
        "    anext, cnext, y, cache = lstm_cell_forward(x[:,:,i], aprev, cprev, parameters)\n",
        "    aprev = anext\n",
        "    cprev = cnext\n",
        "    A[:,:,i] = anext\n",
        "    C[:,:,i] = cnext\n",
        "    Y[:,:,i] = y\n",
        "    \n",
        "    caches.append(cache)\n",
        "    \n",
        "  caches = (caches, x)\n",
        "  return A, Y, C, caches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MQfpLxACMDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "7d70b0b9-0abe-431e-b065-5128a9924a60"
      },
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,7)\n",
        "a0 = np.random.randn(5,10)\n",
        "\n",
        "Wf = np.random.randn(5, 5+3)\n",
        "bf = np.random.randn(5,1)\n",
        "\n",
        "Wu = np.random.randn(5, 5+3)\n",
        "bu = np.random.randn(5,1)\n",
        "\n",
        "Wo = np.random.randn(5, 5+3)\n",
        "bo = np.random.randn(5,1)\n",
        "\n",
        "Wc = np.random.randn(5, 5+3)\n",
        "bc = np.random.randn(5,1)\n",
        "\n",
        "Wy = np.random.randn(2,5)\n",
        "by = np.random.randn(2,1)\n",
        "\n",
        "parameters = {\"Wf\": Wf, \"Wu\": Wu, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bu\": bu, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
        "\n",
        "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
        "print(\"a[4][3][6] = \", a[4][3][6])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y[1][4][3] =\", y[1][4][3])\n",
        "print(\"y.shape = \", y.shape)\n",
        "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
        "print(\"c[1][2][1]\", c[1][2][1])\n",
        "print(\"len(caches) = \", len(caches))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][3][6] =  0.0007505869303062002\n",
            "a.shape =  (5, 10, 7)\n",
            "y[1][4][3] = 0.9089844972350362\n",
            "y.shape =  (2, 10, 7)\n",
            "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
            "  0.41005165]\n",
            "c[1][2][1] 0.03131035093604401\n",
            "len(caches) =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU_wrxMyGVyY",
        "colab_type": "text"
      },
      "source": [
        "## Backpropogation\n",
        "\n",
        "- in basic RNN \n",
        "\n",
        "- In LSTM rnn "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IAahjJGUuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAL7KRyHDut_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}