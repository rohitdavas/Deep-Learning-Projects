# -*- coding: utf-8 -*-
"""fashion_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14uWUakQGskyG3zWZ1AGwZT9Ulyuaw_Nr
"""

#include tensorflow 
import tensorflow as tf

#include keras
from tensorflow import keras

# time to include helper libraries 
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

#import some data to train, here import the MNIST data , you can change the data set to train to your model
fashion_mnist =  keras.datasets.fashion_mnist
(train_images ,train_labels) ,(test_images, test_labels) = fashion_mnist.load_data()

#assign the class names to the labels 0 to 9

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']


# get some knowledge about the train & test images
print(train_images.shape)

print(len(train_labels))
print(train_labels)

x = test_images.shape
y = len(test_labels)
z = test_labels
print(x,'\n',y,'\n',z)

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(True)

train_images = train_images/255.0
test_images = test_images/255.0

plt.figure(figsize = (10,10))
for i in range(40):
    plt.subplot(5,8,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(True)
    plt.xlabel(class_names[train_labels[i]])
    plt.imshow(train_images[i] , cmap = plt.cm.binary)

model = keras.Sequential(
[ keras.layers.Flatten(input_shape = (28, 28)) , keras.layers.Dense(128, activation = tf.nn.relu),keras.layers.Dense(10, activation= tf.nn.softmax)]
)

model.compile( optimizer = tf.train.AdamOptimizer(), loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'] )

model.fit(train_images, train_labels, epochs = 10) #you can increase epochs for increasing the trainining efficiency

test_loss ,test_acc = model.evaluate(test_images, test_labels)
print('test Accuracy',test_acc)

"""t turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. 
**Overfitting** * is when a machine learning model performs worse on new data than on their training data.*
"""

predictions = model.predict(test_images)
predictions[0]

np.argmax(predictions[0])

test_labels[0]

# now let's plot the graph of the predicted image with probability label , color for distinguishing whether it's the right prediction or not.

def plot_image(i, predictions_array, true_label, img):                              #function for image plot
    # assign the values to the variables
    predictions_array, true_label, img = predictions_array[i],true_label[i],img[i]
    plt.grid(False)                                                                   #no use of grid
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img, cmap=plt.cm.binary)
    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
      color = 'blue'
    else:
      color = 'red'
    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                  100*np.max(predictions_array),
                                  class_names[true_label]),
                                  color=color)

  
def plot_value_array(i, predictions_array, true_label):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    thisplot = plt.bar(range(10), predictions_array, color="#777777")
    plt.ylim([0, 1]) 
    predicted_label = np.argmax(predictions_array)

    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')

# let's see the output for 100 images. you can increase as you like .
for i in range(100):  # range(100) = 0 to 100
  plt.figure(figsize=(6,3))
  plt.subplot(1,2,1)
  plot_image(i,predictions,test_labels,test_images)
  plt.subplot(1,2,2)
  plot_value_array(i,predictions, test_labels)

# now let's make prediction for a single image
img = test_images[0]
print(img.shape)

#models are optimized to make predictions on a batch, or collection, 
#of examples at once. So even though we're using a single image, we need to add
#it to a list:

img = (np.expand_dims(img ,0))
print(img.shape)
predictions_single = model.predict(img)
print(predictions_single)
print(np.argmax(predictions_single))

plot_image(0,predictions_single,test_labels,img)
plot_value_array(0, predictions_single, test_labels)

plt.xticks(range(10), class_names, rotation=45)